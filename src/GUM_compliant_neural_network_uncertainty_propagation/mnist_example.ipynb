{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_equal\n",
    "from torch import flatten, load, max, save\n",
    "from torch.cuda import is_available\n",
    "from torch.nn import CrossEntropyLoss, Linear, Module, Sequential, Sigmoid\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST MLP\n",
    "\n",
    "This section serves to get used to creating and training own models. Later this will be transformed into the nets, we use to apply our uncertainty propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset download and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.13, 0.3081)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./train/\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=0.13, std=0.3081)\n           )"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MNIST(root=\"./train/\", download=True, transform=_train_transforms)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n_test_samples = int(0.8 * len(train_data))\n",
    "_n_validat_samples = int(len(train_data) - _n_test_samples)\n",
    "assert _n_test_samples + _n_validat_samples == len(train_data)\n",
    "train_set, validat_set = random_split(train_data, [_n_test_samples, _n_validat_samples])\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "validat_loader = DataLoader(validat_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network design\n",
    "\n",
    "The chosen network design resembles the proposed structure from [stats.stackexchange\n",
    "](https://stats.stackexchange.com/questions/376312/mnist-digit-recognition-what-is-the-best-we-can-get-with-a-fully-connected-nn-o)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_MLP, self).__init__()\n",
    "        self._net = Sequential(\n",
    "            Linear(784, 200), Sigmoid(), Linear(200, 80), Sigmoid(), Linear(80, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        passed_input = flatten(x_0, 1)\n",
    "        return self._net(passed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if is_available() else \"cpu\"\n",
    "model = MNIST_MLP().to(device=device)\n",
    "assert_equal(tuple(model.children())[-1][-1].out_features, len(train_data.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "#### Loss function and optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0de5a97a9464ec8909b6abc77bdb95f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training during epoch 0:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f59f9015cd0346608bab9e563b6e258d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation during epoch 0:   0%|          | 0/750 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2e9e387caea4df4a96e97a0fab4bda0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.40202696987241504, Validation Loss: 0.3769043511946996\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training during epoch 1:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f042f35d50749f094b80d3a0037c972"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation during epoch 1:   0%|          | 0/750 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50338dc2765c4759bee0772478081255"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.346245320511361, Validation Loss: 0.3367252284909288\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training during epoch 2:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ce71aae632246d3b47fb59d51940101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation during epoch 2:   0%|          | 0/750 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d267cae36e4b4e998cc4f683d23a4c3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.31111704791523515, Validation Loss: 0.3112695779800415\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=f\"Epoch {epoch}\"):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for train_x_0s, train_labels in tqdm(\n",
    "        train_loader, desc=f\"Training during epoch {epoch}\"\n",
    "    ):\n",
    "        train_x_0s = train_x_0s.to(device)\n",
    "        train_labels = train_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_x_0s)\n",
    "        tmp_train_loss = loss_func(outputs, train_labels)\n",
    "        tmp_train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += tmp_train_loss.item()\n",
    "\n",
    "    validat_loss = 0.0\n",
    "    model.eval()\n",
    "    for validat_x_0s, validat_labels in tqdm(\n",
    "        validat_loader, desc=f\"Validation during epoch {epoch}\"\n",
    "    ):\n",
    "        validat_x_0s = validat_x_0s.to(device)\n",
    "        validat_labels = validat_labels.to(device)\n",
    "        outputs = model(validat_x_0s)\n",
    "        tmp_validat_loss = loss_func(outputs, validat_labels)\n",
    "        validat_loss += tmp_validat_loss.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, Train Loss: {train_loss/len(train_loader)}, Validation Loss: {validat_loss/len(validat_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "model_path = \"mnist_mlp.pt\"\n",
    "save(model.state_dict(), model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(load(model_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 10000\n    Root location: ./test/\n    Split: Test\n    StandardTransform\nTransform: ToTensor()"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = MNIST(root=\"./test/\", train=False, download=True, transform=ToTensor())\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/625 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb4e45a658414f17a1a44e3a2d92f391"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8238999843597412\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "for x_test_batch, y_test_batch in tqdm(test_loader):\n",
    "    model.eval()\n",
    "    y_test_batch = y_test_batch.to(device)\n",
    "    x_test_batch = x_test_batch.to(device)\n",
    "    y_pred_batch = model(x_test_batch)\n",
    "    _, predicted = max(y_pred_batch, 1)\n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "accuracy = num_correct / (len(test_loader) * test_loader.batch_size)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
